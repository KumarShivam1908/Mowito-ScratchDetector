\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float}
\usepackage[dvipsnames]{xcolor}
\usepackage{microtype}  % Better text alignment and typography
\usepackage{parskip}    % Better paragraph spacing
\usepackage{setspace}   % Line spacing control
\usepackage{titling}    % Title customization

% Set line spacing
\setstretch{1.15}

% Reduce space after title
\setlength{\droptitle}{-2em}
\posttitle{\par\end{center}\vspace{-1em}}

% Custom commands for headings and highlights
\newcommand{\heading}[1]{\vspace{1.5em}\noindent\textbf{\Large #1}\vspace{0.75em}\par\noindent}
\newcommand{\redbold}[1]{\textcolor{red}{\textbf{#1}}}

\title{\textbf{\huge Project Report: Automated Surface Scratch Detection}}
\author{}
\date{}

\begin{document}

\maketitle


\heading{Introduction and Problem Statement}

The objective of this project was to develop a computer vision model capable of identifying surface defects. Specifically, the model is designed to classify input images as either \redbold{``Good'' (defect-free)} or \redbold{``Bad'' (containing scratches)}, leveraging a provided dataset of \redbold{5,180 images} and their corresponding label masks.

\heading{Dataset Analysis and Preprocessing}

A comprehensive analysis of the dataset revealed a \redbold{significant class imbalance} across the 5,180 samples, with 4,177 ``Good'' samples (80.33\%) compared to only 1,023 ``Bad'' samples (19.67\%). This disparity highlighted the immediate need for data balancing to prevent the model from biasing towards the majority class.

To address this class imbalance, a \redbold{targeted augmentation strategy} was implemented. Geometric transformations---including flips, rotations, and translations---were applied exclusively to the minority ``Bad'' class. These specific augmentation techniques were selected because they preserve the essential structural features of the anomalies while generating realistic variations. This process successfully balanced the dataset, ensuring fair and effective downstream model training.

\heading{Model Selection and Experimental Strategy}

Once I had the balanced dataset, I started thinking about the approaches I could use to build the model.

Following the general pattern, I split my dataset into train, validation, and test sets. The validation set was used for model selection between CNN-based and Transformer-based models, with the best model finally evaluated on the test set.

My first thought was, ``Why not just use CNN-based models?'' I could use classical architectures such as ResNet50, MobileNet, and InceptionV3. So, I started working on that.

Before actually trying different models, I fixed the training configuration to ensure a better and fair evaluation.


\textbf{Training Configuration:}
\begin{itemize}
    \item Epochs: 50
    \item Batch size: 32
    \item Learning rate: 0.0001
\end{itemize}

Early Stopping was set up to prevent overfitting and to save the best model weights.

So, I started with ResNet50, InceptionV3, and MobileNetV3. I trained them, and boom, got these graphs:

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-1.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-2.png}
\end{figure}

I noticed distinct behaviors across the three architectures that highlight the trade-offs between speed and stability. \redbold{ResNet50 is the strongest performer} but behaves unusually with validation accuracy consistently beating training accuracy, a classic sign of heavy data augmentation (like CutMix) making the training set ``harder'' than the validation set. In contrast, \redbold{MobileNetV3 proves to be the fastest model ($\approx 5s$ latency)} but struggles with stability, evident in its jagged, ``bouncing'' loss curves, suggesting the learning rate might be too aggressive for its lightweight structure. Finally, \redbold{InceptionV3 is the slowest and shows the clearest signs of overfitting}, where the validation loss begins to rise while training loss continues to fall, indicating it has started memorizing the data rather than learning generalizable features like ResNet did.

While scrolling through the web, I saw multiple Transformer-based models and thought, ``Why not try them out?'' So, I started working on them.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-3.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-4.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-5.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-6.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-7.png}
\end{figure}

\heading{Comparative Analysis: Transformers vs. CNNs}

Comparing the results of both model types yielded surprising findings: \redbold{Transformer-based models generally outperformed their CNN counterparts.}

\textbf{Model Comparison (Validation Set)}

\begin{table}[H]
\centering
\small
\begin{tabular}{lllllll}
\toprule
Model & Type & Accuracy & Precision & Recall & F1 Score & Inference Time (s) \\
\midrule
\textbf{DeiT} & Transformer & \textbf{0.8828} & 0.8101 & \textbf{1.0000} & \textbf{0.8951} & 17.60 \\
Swin Transformer & Transformer & 0.8547 & 0.7916 & 0.9631 & 0.8689 & 18.73 \\
ResNet50 & CNN & 0.8467 & 0.7776 & 0.9711 & 0.8637 & 7.57 \\
DINOv2 & Transformer & 0.8459 & 0.7718 & 0.9823 & 0.8644 & 22.98 \\
ViT-Large & Transformer & 0.8347 & 0.7697 & 0.9551 & 0.8524 & 46.46 \\
ViT-Base & Transformer & 0.7207 & 0.6536 & 0.9390 & 0.7708 & 17.85 \\
InceptionV3 & CNN & 0.6340 & 0.5903 & 0.8764 & 0.7054 & 10.31 \\
MobileNetV3 & CNN & 0.5843 & 0.5618 & 0.7657 & 0.6481 & 5.01 \\
\bottomrule
\end{tabular}
\end{table}

Here are some key insights derived from comparing CNN and Transformer architectures:

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-8.png}
\end{figure}


\clearpage

Below is the confusion matrix for the best-performing Transformer model (DeiT):

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-9.png}
\end{figure}

\heading{Defect Localization and Ensemble Strategy}

With a robust classification model in place, the next logical step was defect localization---identifying the exact region of the scratch. For this task, I employed \redbold{YOLOv11}. To prepare the training data, I utilized the provided label masks to extract the bounding box coordinates of the scratches, converting the dataset into the standard YOLO format.

After training the object detection model, I devised an \redbold{ensemble strategy} to minimize the error rate by combining classification and detection outputs. The pipeline categorizes images into three distinct buckets:

\begin{enumerate}
    \item \textbf{Human Review:} Images that are ambiguous. This includes cases where the classifier predicts ``Good'' but the detector finds a scratch (Good + Scratches), or where confidence scores are low.
    \item \textbf{Bad:} Images confirmed as defective. This includes cases where the classifier predicts ``Bad'' and the detector confirms a scratch with high confidence.
    \item \textbf{Good:} Images confirmed as defect-free. This includes cases where the classifier predicts ``Good'' with high confidence and no scratch is detected.
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-10.png}
\end{figure}

Some sample outputs from the detection model:

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/Code02265.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/Code02265_aug_flip_h.png}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/Code02320.png}
\end{figure}

This multi-stage pipeline approach effectively addressed the challenge of accurate scratch detection, balancing automation with necessary human oversight for edge cases.

\heading{Advanced Data Generation: Beyond Standard Augmentation}

The next major task was to figure out ways to \redbold{increase the dataset size of bad images beyond typical data augmentation techniques.}

There are basically two approaches which came to my mind instantly:

\begin{enumerate}
    \item \redbold{Synthetic Data Generation Using GANs}
    \item \textbf{3D Simulation \& Diffusion:} A novel approach I learned during my internship, which involves creating a 3D model of the scratch, generating edge/depth/segmentation maps, and then passing them to a diffusion-based model to convert the simulated image into a photorealistic one.
\end{enumerate}

For the GAN approach, I experimented with two specific architectures:

\begin{samepage}
\begin{enumerate}
    \item \redbold{StyleGAN2:} I trained this on 5000 images at 64x64 resolution. It took a solid 10 hours, but the results were promising:
    
    \begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{assets/image-11.png}
    \end{figure}
    
    \item \redbold{CycleGAN:} This cyclic discriminator-generator approach allowed for unpaired image-to-image translation, leading to results like this:
    
    \begin{center}
    \includegraphics[width=0.42\textwidth]{assets/img_0460.png}
    \hspace{0.5cm}
    \includegraphics[width=0.42\textwidth]{assets/img_0261.png}
    \end{center}
\end{enumerate}
\end{samepage}

\clearpage
\heading{Deployment and Optimization}

Once the modeling was complete, my main concern was \redbold{how to optimize the inference speed}, which is critical for the deployment stage.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{assets/image-12.png}
\end{figure}

The process begins with a model trained in PyTorch (saved as a \texttt{.pth} file), which is then converted to the \redbold{ONNX format} to make it framework-independent and suitable for deployment. Next, the ONNX model is optimized using \redbold{NVIDIA TensorRT}, which significantly accelerates inference, achieving up to \redbold{2.69× faster speed} compared to the original PyTorch model.

\textbf{Inference Benchmark Results}

Test Images: \textbf{100 images}

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
Model Type & Avg Time (ms) & Median Time (ms) & Std Dev (ms) & FPS & Speedup vs PyTorch \\
\midrule
\textbf{PyTorch} & 9.40 & 8.98 & 1.39 & 106.4 & 1.00× (baseline) \\
\textbf{ONNX} & 8.67 & 8.34 & 0.75 & 115.3 & \textbf{1.08× faster} \\
\textbf{TensorRT} & 3.49 & 3.40 & 0.45 & 286.5 & \textbf{2.69× faster} \\
\bottomrule
\end{tabular}
\end{table}

\clearpage

\heading{Why It Worked: Analysis of Success Factors}

The success of this project can be attributed to several key strategic decisions and methodological choices:

\textbf{1. Data-Centric Approach}

The \redbold{targeted augmentation strategy} was crucial. Rather than applying generic augmentation to all samples, I focused exclusively on the minority ``Bad'' class. This approach:
\begin{itemize}
    \item Preserved the natural distribution of ``Good'' samples for better real-world representation
    \item Generated meaningful variations of defects while maintaining their structural characteristics
    \item Successfully balanced the dataset without introducing artificial bias
\end{itemize}

\textbf{2. Architecture Selection Philosophy}

The decision to explore \redbold{both CNNs and Transformers} proved invaluable. The comparative analysis revealed that:
\begin{itemize}
    \item \redbold{Transformers excel at capturing global context}, which is critical for detecting subtle scratches across an entire surface
    \item DeiT's attention mechanism could focus on fine-grained defect patterns that CNNs might miss
    \item The \redbold{100\% recall on the ``Bad'' class} demonstrates that Transformers effectively learned to identify all defective samples
\end{itemize}

\textbf{3. Transfer Learning and Pre-training}

Leveraging \redbold{pre-trained weights} from models trained on ImageNet provided:
\begin{itemize}
    \item Strong feature extractors that could generalize to scratch detection
    \item Faster convergence during training (50 epochs vs. potentially 100+ from scratch)
    \item Better performance with limited data (5,180 images is relatively small for deep learning)
\end{itemize}

\textbf{4. Ensemble Strategy}

The \redbold{combination of classification and localization} models created a robust system:
\begin{itemize}
    \item Classification provides high-level decision making
    \item Localization adds spatial verification and interpretability
    \item Human-in-the-loop for ambiguous cases prevents costly false negatives
\end{itemize}

\textbf{5. Deployment Optimization}

The \redbold{TensorRT optimization} achieved \redbold{2.69× speedup} because:
\begin{itemize}
    \item Layer fusion reduced memory bandwidth requirements
    \item Kernel auto-tuning optimized operations for specific GPU hardware
    \item Precision calibration (FP16) maintained accuracy while improving throughput
\end{itemize}

\textbf{What Didn't Work as Expected:}

\begin{itemize}
    \item \redbold{MobileNetV3 showed instability} - The lightweight architecture struggled with the learning rate, suggesting that smaller models require more careful hyperparameter tuning for this task
    \item \redbold{InceptionV3 overfitted} - Despite its sophisticated architecture, it failed to generalize, likely due to its complexity relative to dataset size
    \item \redbold{StyleGAN2 at 64×64 resolution} - While promising, the low resolution limited practical applicability; higher resolutions would require significantly more compute
\end{itemize}

\clearpage

\heading{Future Improvements: With More Time and Compute}

Given additional resources, I would pursue the following directions to further enhance the system:

\textbf{1. Advanced Model Architectures}

\begin{itemize}
    \item \redbold{Vision Transformer variants}: Explore newer architectures like \redbold{DeiT-III, BEiT, or MAE} which have shown superior performance on fine-grained visual tasks
    \item \redbold{Hierarchical Vision Transformers}: Models like Swin V2 or MaxViT that can capture both local and global features more efficiently
    \item \redbold{Efficient Transformers}: Investigate models like EfficientFormer or MobileViT for better speed-accuracy trade-offs in deployment
\end{itemize}

\textbf{2. Self-Supervised and Semi-Supervised Learning}

With more compute, I would implement:
\begin{itemize}
    \item \redbold{Masked Autoencoder (MAE) pre-training} on unlabeled scratch images to learn better representations before fine-tuning
    \item \redbold{Contrastive learning} (SimCLR, MoCo) to learn discriminative features from augmented views of scratch patterns
    \item \redbold{Pseudo-labeling} to leverage large amounts of unlabeled industrial inspection data
\end{itemize}

\textbf{3. Enhanced Data Generation}

\begin{itemize}
    \item \redbold{High-resolution StyleGAN3} (256×256 or 512×512) with progressive growing for photorealistic synthetic scratches
    \item \redbold{Diffusion models} (Stable Diffusion, DALL-E) fine-tuned on scratch patterns for controllable synthetic data generation
    \item \redbold{3D simulation pipeline}: Create physically-based scratch models with realistic lighting and material properties, then render using path tracing
    \item \redbold{Domain randomization}: Vary lighting, surface textures, and camera angles to improve model robustness
\end{itemize}

\textbf{4. Advanced Training Techniques}

\begin{itemize}
    \item \redbold{Focal Loss} to better handle class imbalance and focus on hard examples
    \item \redbold{Knowledge distillation} from the large DeiT model to a smaller, faster student model for deployment
    \item \redbold{Multi-task learning}: Jointly train on classification, segmentation, and localization for better feature sharing
    \item \redbold{Test-time augmentation (TTA)} to improve inference accuracy through ensemble predictions
\end{itemize}

\textbf{5. Deployment and Production Optimizations}

\begin{itemize}
    \item \redbold{INT8 quantization} on TensorRT for potentially \redbold{3--5× additional speedup} with minimal accuracy loss
    \item \redbold{Knowledge distillation} to create a lightweight student model (e.g., MobileNetV3) that matches DeiT's accuracy
    \item \redbold{ONNX Runtime optimizations} with dynamic quantization and graph optimizations
    \item \redbold{Edge deployment}: Optimize models for deployment on edge devices (NVIDIA Jetson, Intel NUC) for real-time factory inspections
    \item \redbold{Multi-GPU and batched inference} for high-throughput scenarios
\end{itemize}

\textbf{6. Model Interpretability and Robustness}

\begin{itemize}
    \item \redbold{Attention visualization} to understand what features the Transformer focuses on when detecting scratches
    \item \redbold{Grad-CAM and SHAP} analysis for better interpretability and debugging
    \item \redbold{Adversarial training} to improve robustness against variations in lighting, surface reflections, and camera noise
    \item \redbold{Uncertainty quantification} using techniques like Monte Carlo Dropout or Deep Ensembles to provide confidence intervals with predictions
\end{itemize}

\textbf{7. Extended Ensemble and Active Learning}

\begin{itemize}
    \item \redbold{Model ensembles}: Combine DeiT, Swin, and ResNet50 predictions using weighted voting or stacking
    \item \redbold{Active learning pipeline}: Automatically identify ambiguous cases for human labeling to continuously improve the model
    \item \redbold{Online learning}: Update models periodically with new production data to adapt to changing scratch patterns
\end{itemize}

\textbf{8. Real-World Deployment Considerations}

\begin{itemize}
    \item \redbold{A/B testing framework} to gradually roll out new models while monitoring performance
    \item \redbold{Model monitoring}: Track inference latency, accuracy drift, and data distribution shifts in production
    \item \redbold{Fallback mechanisms}: Implement graceful degradation when models are uncertain
    \item \redbold{Multi-stage pipeline}: Use fast CNN models for initial screening, then deploy expensive Transformer models only on borderline cases
\end{itemize}

\end{document}
